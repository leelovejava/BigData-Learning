spark-streaming:
SPark Streaming用于对流式进行处理，类似于Storm。
Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。

Spark Streaming吞吐量高，和spark的扩展性：和Spark Core、Spark SQL无缝整合，实时处理出来的中间数据
Storm--纯实时，Spark Streaming---准实时(RDD转换)。Storm支持事务机制、健壮性 / 容错性、动态调整并行度等特性。

场景:
对于Storm来说：
1、纯实时--实时金融系统，金融交易和分析
2、事务机制和可靠性机制
3、如果还需要针对高峰低峰时间段，动态调整实时计算程序的并行度，以最大限度利用集群资源（通常是在小型公司，集群资源紧张的情况），也可以考虑用Storm
4、不需要在中间执行SQL交互式查询、复杂的transformation算子等，那么用Storm是比较好的选择

对于Spark Streaming来说：
1、还包括了离线批处理、交互式查询等业务功能
2、涉及到高延迟批处理、交互式查询等功能

怎么实现的:
1、Spark Streaming 采用“微批次”架构。
2、对于整个流式计算来说，数据流你可以想象成水流，微批次架构的意思就是将水流按照用户设定的时间间隔分割为多个水流段。一个段的水会在Spark中转换成为一个RDD，所以对水流的操作也就是对这些分割后的RDD进行单独的操作。每一个RDD的操作都可以认为是一个小的批处理（也就是离线处理）。

spark-streaming使用“微批次” 把流式计算当作一系列连续的小规模批处理，spark-spark-streaming
从各种输入源中读取数据，并把数据分组为小批次，每个批次间隔500毫秒
DStream:离散化流,它是一个RDD序列,每个RDD代表数据流中的一个时间片的数据

怎么用：
一、创建streamcontext
var conf=new SparkConf().setMaster("local[3]").setAppName("socket-test")
val ssc=new StreamingContext(conf,Seconds(1))
Spark Streaming 的输入:
1.文件通过:streamingContext.textFileStream(dataDirectory)
2.自定义输入：
extends Receiver[String](StorageLevel.MEMORY_AND_DISK)
重写:onStart方法（在Receiver启动的时候调用的方法）、onStop方法（在Receiver正常停止的情况下调用的方法）
涉及多线程,sock(获取ip,端口,交给输入流调用),输入流调用时： val lines=  ssc.receiverStream(new socketReceiver("hadoop100",9999))

wordcount:
val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(1))

// Create a DStream that will connect to hostname:port, like localhost:9999
val lines = ssc.receiverStream(new CustomReceiver("master01", 9999))

// Split each line into words
val words = lines.flatMap(_.split(" "))

//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3
// Count each word in each batch
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

// Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.print()

ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate
//ssc.stop()

二、DStream转换：
包括无状态转化和有状态转化
无状态转化：map(),flatMap(),filit(),repartition()
有状态转化(根据之前的RDD或中间数据生成当前RDD)：updateStateBykey() window系列的
updateStateBykey():结果贯穿整个应用程序,需要做检查点的目录
window系列，如reduceByKeyAndWindow 是一个时间范围的计算,处理一段时间内发送的业务,如：性能,比例 
		包括:窗口大小：类似于1步
			 滑动步长：类似于1步的大小
			 
输出：
print、saveAsTextFiles、saveAsHadoopFiles













