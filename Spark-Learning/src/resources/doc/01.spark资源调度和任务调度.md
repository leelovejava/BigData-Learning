# Spark资源调度和任务调度

## 1. 术语解释

* Master(standalone): 资源管理的主节点(进程)

* Cluster Manager: 在集群上获取资源的外部服务,运行模式(例如:`standalone`,`Mesos`,`Yarn`)

* Worker Node(standalone): 资源管理的从节点(进程)或者说管理本机资源的进程

* Application: 基于Spark的应用程序,包含了driver程序和运行在集群上的executor程序

* Driver Program: 用于连接工作进程(Worker)的程序.

driver进程就是应用的main()函数并且构建sparkContext对象，当我们提交了应用之后，便会启动一个对应的driver进程，driver本身会根据我们设置的参数占有一定的资源（主要指cpu core和memory）

* Executor: 是在一个worker进程所管理的节点上为某个Application启动的一个线程,该进程负责运行任务,并且负责将数据存在内存或磁盘上.每个应用程序都有各自独立的`executors`

* Task: 被送到某个executor上的工作单位

* Job: 包含很多任务(Task)的并行计算,可以看做和action对应

* Stage: 一个Job会被拆分很多组任务(Stage由一组并行的task组成),每组任务被称为Stage(就像MapReduce分map task和reduce task一样)


一个master节点，两个worker节点，并且驱动程序运行在master节点上

在master节点提交应用以后，在master节点中启动driver进程，driver进程向集群管理者申请资源（executor），集群管理者在不同的worker上启动了executor进程，相当于分配资源

driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行

## 2. 窄依赖和宽依赖
RDD之间有一系列的依赖关系，依赖关系又分为窄依赖和宽依赖。

窄依赖

父RDD和子RDD partition之间的关系是一对一的。或者父RDD一个partition只对应一个子RDD的partition情况下的父RDD和子RDD partition关系是多对一的。不会有shuffle的产生。

宽依赖

父RDD与子RDD partition之间的关系是一对多。会有shuffle的产生。    

## 3.Stage
Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分stage的依据就是RDD之间的宽窄依赖。

遇到宽依赖就划分stage,每个stage包含一个或多个task任务。然后将这些task以taskSet的形式提交给TaskScheduler运行。
    
  stage是由一组并行的task组成。

stage切割规则

切割规则：从后往前，遇到宽依赖就切割stage。


stage计算模式

pipeline管道计算模式,`pipeline`只是一种计算思想，模式。

计算模式为: 每个task相当于执行了一个高阶函数,f4(f3(f2(f1()))),以上这种计算模式就是`pipeline`的计算模式

数据一直在管道里面什么时候数据会落地？
    1.对RDD进行持久化。
    2.shuffle write的时候。
    Stage的task并行度是由stage的最后一个RDD的分区数来决定的 。
    
如何改变RDD的分区数？

   例如：reduceByKey(XXX,3),GroupByKey(4)

测试验证pipeline计算模式

```scala
val conf = new SparkConf()
conf.setMaster("local").setAppName("pipeline");
val sc = new SparkContext(conf)
val rdd = sc.parallelize(Array(1,2,3,4))
val rdd1 = rdd.map { x => {
  println("map--------"+x)
  x
}}

val rdd2 = rdd1.filter { x => {
  println("fliter********"+x)
  true
} }

rdd2.collect()
sc.stop()
```

## 4.Spark资源调度和任务调度

Spark资源调度和任务调度的流程：

启动集群后，Worker节点会向Master节点汇报资源情况，Master掌握了集群资源情况。

当Spark提交一个Application后，根据RDD之间的依赖关系将Application形成一个DAG有向无环图。

任务提交后，Spark会在Driver端创建两个对象：DAGScheduler和TaskScheduler，DAGScheduler是任务调度的高层调度器，是一个对象。

DAGScheduler的主要作用就是将DAG根据RDD之间的宽窄依赖关系划分为一个个的Stage，然后将这些Stage以TaskSet的形式提交给TaskScheduler

（TaskScheduler是任务调度的低层调度器，这里TaskSet其实就是一个集合，里面封装的就是一个个的task任务,也就是stage中的并行度task任务），

`TaskSchedule`会遍历TaskSet集合，拿到每个task后会将task发送到计算节点Executor中去执行（其实就是发送到Executor中的线程池ThreadPool去执行）。

task在Executor线程池中的运行情况会向TaskScheduler反馈，当task执行失败时，则由TaskScheduler负责重试，将task重新发送给Executor去执行，默认重试3次。

如果重试3次依然失败，那么这个task所在的`stage`就失败了。stage失败了则由`DAGScheduler`来负责重试，重新发送TaskSet到`TaskSchdeuler`，Stage默认重试4次。

如果重试4次以后依然失败，那么这个job就失败了。

job失败了，Application就失败了。

TaskScheduler不仅能重试失败的task,还会重试straggling（落后，缓慢）task（也就是执行速度比其他task慢太多的task）。

如果有运行缓慢的task那么TaskScheduler会启动一个新的task来与这个运行缓慢的task执行相同的处理逻辑。

两个task哪个先执行完，就以哪个task的执行结果为准。这就是Spark的推测执行机制。

在Spark中推测执行默认是关闭的。推测执行可以通过`spark.speculation`属性来配置。

注意：

对于ETL类型要入数据库的业务要关闭推测执行机制，这样就不会有重复的数据入库。

如果遇到数据倾斜的情况，开启推测执行则有可能导致一直会有task重新启动处理相同的逻辑，任务可能一直处于处理不完的状态。

图解Spark资源调度和任务调度的流程


粗粒度资源申请和细粒度资源申请

粗粒度资源申请(Spark）

在Application执行之前，将所有的资源申请完毕，当资源申请成功后，才会进行任务的调度，当所有的task执行完成后，才会释放这部分资源。

优点：在Application执行之前，所有的资源都申请完毕，每一个task直接使用资源就可以了，不需要task在执行前自己去申请资源，task启动就快了，task执行快了，stage执行就快了，job就快了，application执行就快了。

缺点：直到最后一个task执行完成才会释放资源，集群的资源无法充分利用。

细粒度资源申请（MapReduce）

Application执行之前不需要先去申请资源，而是直接执行，让job中的每一个task在执行前自己去申请资源，task执行完成就释放资源。

优点：集群的资源可以充分利用。

缺点：task自己去申请资源，task启动变慢，Application的运行就相应的变慢了。