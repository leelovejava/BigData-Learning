RDD 是分布式数据集:它就是一个class
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {
继承了Serializable 和具有Logging的特质
为什么要Serializable：因为不同的RDD之间需要进行转化(序列化:数据转化成二进制,二进制转化为数据)
这也是RDD的五大属性之一,可以进行分区,相互转化,kv结构的,计算split,可以在并行下操作:比如基础操作 map filter persist
kv操作groupbykey jion  

RDD的依赖关系
窄依赖（narrow dependency）父依赖只有一个，出度1
宽依赖（wide dependency）父依赖有多个，出度大于2
区别：是否要进行shuffle阶段（合并分区的过程）

简单理解：
窄依赖:一个RDD对于他的父RDD,只有简单的一对一的依赖关系，RDD的每个partition，仅仅依赖于父RDD的一个partition
父RDD和子RDD的partition之间 对于关系 一对一

宽依赖：shuffle Dependency 本质就是shuffle,父RDD和子RDD的partition之间 错综复杂

Stage的划分：由后往前倒推算宽依赖，遇到窄依赖 +1循环：宽依赖-窄依赖依次循环
宽依赖和窄依赖针对的是对象的出度（一个RDD的父类有入度有多少）
Transformation和Action针对的是过程结果 只有Action时才会进行执行（有一个懒执行--优化）

RDD持久化:每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用
是什么:每一个节点都将把计算的分片结果保存在内存中
为什么：加快效率，防止重新计算浪费资源
怎么办：persist() cache()
cache内部调用了persist方法调用的是MEMORY_ONLY级别。
persist()  默认把数据以序列化的形式缓存在jvm的堆空间中 
// 设置缓存级别：MEMORY_ONLY,MEMORY_ONLY_SER
data.persist(StorageLevel.DISK_ONLY)
// 清除缓存
data.unpersist
//data.unpersist(blocking=true)
持久化级别按照：存储的位置（磁盘、内存、非堆内存）、是否序列化、存储的份数（1,2）进行划分



chackpoint:本质是通过将RDD写入Disk做检查点(保存到hdfs上)，血统是否恢复(源码写着不能指向父类RDD)
cache 和 checkpoint区别：
cache把RDD计算出来然后放在内存中，宕机的时候
checkpoint是把RDD保存在HDFS中，多副本，实现的高容错,相对由内存存储到磁盘占用的资源也增加
所以：
一个job的RDD 我们存储在cache() 调用的是persist()的MEMORY_ONLY，persist()中
若多个RDD使用的资源会使用checkpoint() 比如:基础性的数据

使用操作:
使用sc.setCheckpointDir("hdfs://master01:9000/checkpoint")----指定存储目录
	data.checkpoint------进行checkpoint()操作


RDD的数据分区 默认Hash分区
hash分区:对于给定的key，计算其hashCode，并除于分区的个数取余--容易造成数据倾斜
range分区:采用的是水塘抽样算法：将数据一部分放在分区中，避免了一个数据倾斜的状态
自定义分区：
1.extends Partitioner 类似于hadoop中shuffle阶段的Partition
2.重写partition方法:传入传入参数为(key: Any) 调用:RDD.partitionBy(new CustomerPartitioner(分区数))

scala 获取分区数的元素：res3.mapPartitionsWithIndex((index,iter)=>Iterator(index+"===== "+iter.mkString(" , "))).collect


RDD 累加器:线程安全,不是针对某个节点或者某个RDD的，它的对象是整个Spark 类似于hadoop的累加器
进行累加操作,不仅仅计数,还可以进行简单的过滤
RDD.count()

自定义累加器
继承AccumulatorV2,要自己写存储容器:包括简单的增删改查,累加累减，清空等待
调用的使用创建对应的自定义对象进行调用
最主要的是要进行sc（SparkContext()）注册:
val accum = new LogAccumulator
sc.register(accum, "logAccum")

广播变量：解决只读大对象，解决内存和分发的负载

注意：当你在RDD中使用到了class的方法或者属性的时候，该class需要继承java.io.Serializable接口，或者可以将属性赋值为本地变量来防止整个对象的传输。

创建 parallepel( 1 to 10,3)----3个并行度

保存结果:
1、文本文件它的输入输出：textFile和saveAsTextFile----在写出text文件的时候,每一个partition会单独写出,文件系统支持所有和Hadoop文件系统兼容的文件系统

2、JSON文件或者CSV文件:
	这种有格式的文件的输入和输出还是通过文本文件的输入和输出来支持的，Spark Core没有内置对JSON文件和CSV文件的解析和反解析功能，这个解析功能是需要用户自己根据需求来定制的。 注意：JSON文件的读取如果需要多个partition来读，那么JSOn文件一般一行一个json。如果你的JSON是跨行的，那么需要整体读入所有数据，并整体解析。

3、Sequence文件--sc.sequenceFile:HDFS中的文件block数为1，那么Spark设定了最小的读取partition数为2

4、读取或者保存ObjectFile---sc.saveAsObjectFile 输出的是对象的形式
     1、对于ObjectFile他的读取和保存使用了读取和保存SequenceFile的API，也最终调用了hadoop的API。
     2、ObjectFile的读取使用 objectFile进行。
     3、ObjectFile的输出直接使用  saveAsObjectFile来进行输出。
     4、需要注意的是，在读取ObjectFile的时候需要制定对象的类型，而并不是K-V的类型。

5.HadoopAPI的读取和输入：
读取：newApiHadoopFile和newApiHadoopRDD两个方法，最终都是调用newApiHadoopRDD来进行实现。
输出：saveAsNewApiHadoopFile和saveAsNewApiHadoopDataset两个方法，最终都是调用saveAsNewApiHadoopDataset这个方法进行实现。
比旧的hadoopApi 主要进行可以 压缩存储文件到HDFS--进行了优化
     

6.关系型数据库的输入输出：JdbcRDD 里面包括了驱动，数据库用户名/密码
     1、对于关系型数据库的读取使用JdbcRDD来进行实现，JdbcRDD需要依次传入sparkContext、获取JDBC Connection的无参方法、查询数据库的SQL语句，id的下界、id的上界、分区数、提供解析Result Set的函数。
     2、对于关系型数据库的输出，直接采用jdbc执行Insert语句或者Update语句进行实现。




