2019-06-16 14:10:23,009   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:10:24,033   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:10:24,179   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:10:24,181   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:10:24,182   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:10:24,183   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:10:24,184   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:10:25,235   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60700.
2019-06-16 14:10:25,273   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:10:25,307   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:10:25,313   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:10:25,329   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:10:25,402   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-a45c0be5-2430-4498-b817-5e31187e93d6
2019-06-16 14:10:25,443   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:10:25,468   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:10:25,593   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4188ms
2019-06-16 14:10:25,687   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:10:25,716   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @4313ms
2019-06-16 14:10:25,756   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1eaae130{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:10:25,757   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:10:25,803   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,804   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,805   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,811   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,815   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,816   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,817   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,819   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,820   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,833   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,833   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,835   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:10:25,838   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:10:26,008   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:10:26,045   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60713.
2019-06-16 14:10:26,046   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:60713
2019-06-16 14:10:26,048   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:10:26,051   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 60713, None)
2019-06-16 14:10:26,055   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:60713 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 60713, None)
2019-06-16 14:10:26,061   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 60713, None)
2019-06-16 14:10:26,062   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 60713, None)
2019-06-16 14:10:26,286   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:10:27,083   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:10:27,203   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:10:27,207   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:60713 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:10:27,215   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:12
2019-06-16 14:10:27,474   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:10:27,500   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1eaae130{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:10:27,503   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:10:27,515   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:10:27,536   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:10:27,537   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:10:27,543   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:10:27,547   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:10:27,552   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:10:27,552   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:10:27,554   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c69f9a12-d10a-4c87-80c8-9a4561c84890
2019-06-16 14:11:59,217   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:11:59,914   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:12:00,026   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:12:00,028   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:12:00,030   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:12:00,031   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:12:00,032   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:12:00,910   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60765.
2019-06-16 14:12:00,942   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:12:00,970   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:12:00,974   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:12:00,975   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:12:00,989   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6a7c10c7-54fb-452e-bcf4-706108b1814c
2019-06-16 14:12:01,023   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:12:01,043   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:12:01,145   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3406ms
2019-06-16 14:12:01,217   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:12:01,236   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3499ms
2019-06-16 14:12:01,260   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6bda1d19{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:12:01,260   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:12:01,293   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2caa5d7c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,294   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@61af1510{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3901f6af{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,301   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,307   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,308   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7487b142{/static,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bfc14b9{/api,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@55d9b8f0{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:12:01,327   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:12:01,484   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:12:01,517   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60778.
2019-06-16 14:12:01,518   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:60778
2019-06-16 14:12:01,519   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:12:01,522   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 60778, None)
2019-06-16 14:12:01,526   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:60778 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 60778, None)
2019-06-16 14:12:01,531   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 60778, None)
2019-06-16 14:12:01,531   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 60778, None)
2019-06-16 14:12:01,728   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@514de325{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:12:02,315   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:12:02,408   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:12:02,412   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:60778 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:12:02,418   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:12
2019-06-16 14:12:02,555   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:12:02,561   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6bda1d19{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:12:02,564   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:12:02,576   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:12:02,585   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:12:02,586   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:12:02,592   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:12:02,597   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:12:02,601   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:12:02,601   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:12:02,602   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-114c79e8-ebde-46a5-ad66-c4daaa23df17
2019-06-16 14:13:19,842   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:13:20,595   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:13:20,705   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:13:20,707   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:13:20,707   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:13:20,708   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:13:20,708   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:13:21,628   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60834.
2019-06-16 14:13:21,663   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:13:21,691   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:13:21,695   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:13:21,696   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:13:21,710   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d93324f1-91f4-4e41-9a1c-602971a4ef2b
2019-06-16 14:13:21,746   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:13:21,766   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:13:21,877   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3579ms
2019-06-16 14:13:21,954   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:13:21,972   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3676ms
2019-06-16 14:13:21,998   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1b898eb4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:13:21,998   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:13:22,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,042   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,042   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,043   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,046   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,047   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,048   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,049   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,058   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,059   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,061   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,062   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,063   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:13:22,065   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:13:22,223   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:13:22,259   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60847.
2019-06-16 14:13:22,260   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:60847
2019-06-16 14:13:22,262   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:13:22,264   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 60847, None)
2019-06-16 14:13:22,268   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:60847 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 60847, None)
2019-06-16 14:13:22,273   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 60847, None)
2019-06-16 14:13:22,274   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 60847, None)
2019-06-16 14:13:22,481   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:13:23,156   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:13:23,285   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:13:23,289   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:60847 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:13:23,296   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:12
2019-06-16 14:13:23,448   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 14:13:23,538   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:20
2019-06-16 14:13:24,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:14)
2019-06-16 14:13:24,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at SparkScalaWordCount.scala:18)
2019-06-16 14:13:24,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:20) with 1 output partitions
2019-06-16 14:13:24,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:20)
2019-06-16 14:13:24,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 14:13:24,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 14:13:24,028   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:14), which has no missing parents
2019-06-16 14:13:24,114   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 14:13:24,117   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 14:13:24,118   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:60847 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 14:13:24,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:13:24,140   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:14) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:13:24,141   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 14:13:24,203   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 14:13:24,214   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 14:13:24,293   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 14:13:24,429   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 14:13:24,443   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 256 ms on localhost (executor driver) (1/1)
2019-06-16 14:13:24,447   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 14:13:24,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:14) finished in 0.403 s
2019-06-16 14:13:24,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:13:24,465   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:13:24,466   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 14:13:24,466   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:13:24,471   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:18), which has no missing parents
2019-06-16 14:13:24,489   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2019-06-16 14:13:24,492   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2019-06-16 14:13:24,493   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:60847 (size: 2.4 KB, free: 4.1 GB)
2019-06-16 14:13:24,494   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:13:24,496   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:18) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:13:24,496   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 14:13:24,509   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 14:13:24,509   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 14:13:24,534   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:13:24,537   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-06-16 14:13:24,600   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1328 bytes result sent to driver
2019-06-16 14:13:24,603   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 96 ms on localhost (executor driver) (1/1)
2019-06-16 14:13:24,603   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 14:13:24,605   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (map at SparkScalaWordCount.scala:18) finished in 0.126 s
2019-06-16 14:13:24,605   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:13:24,605   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:13:24,605   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 14:13:24,605   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:13:24,606   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at map at SparkScalaWordCount.scala:20), which has no missing parents
2019-06-16 14:13:24,611   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 4.1 GB)
2019-06-16 14:13:24,613   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 4.1 GB)
2019-06-16 14:13:24,614   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:60847 (size: 2.3 KB, free: 4.1 GB)
2019-06-16 14:13:24,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:13:24,617   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at SparkScalaWordCount.scala:20) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:13:24,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 14:13:24,621   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 14:13:24,621   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 14:13:24,628   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:13:24,628   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-06-16 14:13:24,662   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1181 bytes result sent to driver
2019-06-16 14:13:24,664   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)
2019-06-16 14:13:24,664   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 14:13:24,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:20) finished in 0.056 s
2019-06-16 14:13:24,673   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:20, took 1.134495 s
2019-06-16 14:13:24,682   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:13:24,690   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1b898eb4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:13:24,692   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:13:24,706   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:13:24,720   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:13:24,721   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:13:24,728   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:13:24,731   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:13:24,735   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:13:24,736   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:13:24,737   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-d4380fdc-910b-49d8-bc22-91d3e3aedc55
2019-06-16 14:14:19,895   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:14:20,660   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:14:20,782   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:14:20,785   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:14:20,786   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:14:20,787   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:14:20,789   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:14:21,740   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60897.
2019-06-16 14:14:21,773   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:14:21,801   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:14:21,805   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:14:21,806   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:14:21,819   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-dc922ef6-6619-4eee-b99b-6d05c90842fd
2019-06-16 14:14:21,852   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:14:21,872   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:14:21,966   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3528ms
2019-06-16 14:14:22,036   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:14:22,053   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3617ms
2019-06-16 14:14:22,075   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@7acd6f6f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:14:22,075   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:14:22,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,111   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,112   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,112   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,114   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,118   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,121   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,121   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,123   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,124   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,131   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,131   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,133   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,133   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,134   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:14:22,136   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:14:22,314   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:14:22,346   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60910.
2019-06-16 14:14:22,347   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:60910
2019-06-16 14:14:22,349   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:14:22,351   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 60910, None)
2019-06-16 14:14:22,355   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:60910 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 60910, None)
2019-06-16 14:14:22,360   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 60910, None)
2019-06-16 14:14:22,361   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 60910, None)
2019-06-16 14:14:22,581   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:14:23,205   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:14:23,303   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:14:23,306   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:60910 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:14:23,313   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:13
2019-06-16 14:14:23,446   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 14:14:23,502   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:21
2019-06-16 14:14:23,868   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:15)
2019-06-16 14:14:23,870   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at SparkScalaWordCount.scala:19)
2019-06-16 14:14:23,872   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:21) with 1 output partitions
2019-06-16 14:14:23,873   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:21)
2019-06-16 14:14:23,873   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 14:14:23,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 14:14:23,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:15), which has no missing parents
2019-06-16 14:14:23,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 14:14:23,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 14:14:23,965   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:60910 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 14:14:23,966   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:14:23,988   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:15) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:14:23,989   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 14:14:24,045   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 14:14:24,062   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 14:14:24,156   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 14:14:24,267   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 14:14:24,279   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 248 ms on localhost (executor driver) (1/1)
2019-06-16 14:14:24,282   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 14:14:24,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:15) finished in 0.380 s
2019-06-16 14:14:24,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:14:24,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:14:24,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 14:14:24,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:14:24,297   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:19), which has no missing parents
2019-06-16 14:14:24,316   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.1 KB, free 4.1 GB)
2019-06-16 14:14:24,318   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 4.1 GB)
2019-06-16 14:14:24,320   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:60910 (size: 2.4 KB, free: 4.1 GB)
2019-06-16 14:14:24,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:14:24,322   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:19) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:14:24,322   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 14:14:24,328   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 14:14:24,328   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 14:14:24,350   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:14:24,353   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-06-16 14:14:24,409   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1328 bytes result sent to driver
2019-06-16 14:14:24,412   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 86 ms on localhost (executor driver) (1/1)
2019-06-16 14:14:24,412   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 14:14:24,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (map at SparkScalaWordCount.scala:19) finished in 0.110 s
2019-06-16 14:14:24,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:14:24,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:14:24,415   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 14:14:24,415   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:14:24,415   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at map at SparkScalaWordCount.scala:21), which has no missing parents
2019-06-16 14:14:24,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 4.1 GB)
2019-06-16 14:14:24,422   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 4.1 GB)
2019-06-16 14:14:24,423   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:60910 (size: 2.3 KB, free: 4.1 GB)
2019-06-16 14:14:24,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:14:24,427   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at map at SparkScalaWordCount.scala:21) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:14:24,427   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 14:14:24,430   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 14:14:24,431   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 14:14:24,440   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:14:24,441   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 14:14:24,477   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1181 bytes result sent to driver
2019-06-16 14:14:24,479   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 51 ms on localhost (executor driver) (1/1)
2019-06-16 14:14:24,479   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 14:14:24,481   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:21) finished in 0.064 s
2019-06-16 14:14:24,489   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:21, took 0.985673 s
2019-06-16 14:14:24,496   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:14:24,503   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@7acd6f6f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:14:24,506   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:14:24,518   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:14:24,538   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:14:24,539   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:14:24,546   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:14:24,549   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:14:24,554   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:14:24,555   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:14:24,556   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-25a27769-2e97-47ab-88ff-ea54ea48319a
2019-06-16 14:52:41,086   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:52:41,808   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:52:41,936   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:52:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:52:41,939   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:52:41,939   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:52:41,940   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:52:42,885   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61306.
2019-06-16 14:52:42,916   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:52:42,941   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:52:42,946   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:52:42,946   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:52:42,959   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-eaa549c5-9b32-4da2-a230-ddb62d11ee39
2019-06-16 14:52:42,991   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:52:43,009   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:52:43,101   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3396ms
2019-06-16 14:52:43,167   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:52:43,185   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3481ms
2019-06-16 14:52:43,207   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@f08fdce{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:52:43,207   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:52:43,238   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,239   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,239   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,241   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,242   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,242   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,245   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,246   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,246   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,247   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,248   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,250   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,250   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,250   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,251   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,253   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,254   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,262   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/static,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,262   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,264   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/api,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:52:43,268   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:52:43,423   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:52:43,454   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61319.
2019-06-16 14:52:43,455   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:61319
2019-06-16 14:52:43,457   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:52:43,458   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 61319, None)
2019-06-16 14:52:43,461   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:61319 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 61319, None)
2019-06-16 14:52:43,466   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 61319, None)
2019-06-16 14:52:43,466   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 61319, None)
2019-06-16 14:52:43,652   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3b48e183{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:52:44,219   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:52:44,312   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:52:44,315   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:61319 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:52:44,322   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:47
2019-06-16 14:52:44,461   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 14:52:44,507   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:50
2019-06-16 14:52:44,882   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:49)
2019-06-16 14:52:44,883   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 4 (reduceByKey at SparkScalaWordCount.scala:49)
2019-06-16 14:52:44,885   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:50) with 1 output partitions
2019-06-16 14:52:44,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:50)
2019-06-16 14:52:44,887   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 14:52:44,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 14:52:44,899   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49), which has no missing parents
2019-06-16 14:52:44,969   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 14:52:44,972   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 14:52:44,973   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:61319 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 14:52:44,974   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:52:44,994   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:52:44,995   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 14:52:45,045   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 14:52:45,054   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 14:52:45,125   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 14:52:45,231   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 14:52:45,240   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 209 ms on localhost (executor driver) (1/1)
2019-06-16 14:52:45,243   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 14:52:45,250   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:49) finished in 0.329 s
2019-06-16 14:52:45,251   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:52:45,251   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:52:45,252   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 14:52:45,252   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:52:45,257   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ShuffledRDD[4] at reduceByKey at SparkScalaWordCount.scala:49), which has no missing parents
2019-06-16 14:52:45,272   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 4.1 GB)
2019-06-16 14:52:45,275   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.1 GB)
2019-06-16 14:52:45,276   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:61319 (size: 2.2 KB, free: 4.1 GB)
2019-06-16 14:52:45,277   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:52:45,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ShuffledRDD[4] at reduceByKey at SparkScalaWordCount.scala:49) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:52:45,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 14:52:45,284   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 14:52:45,285   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 14:52:45,310   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:52:45,314   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 13 ms
2019-06-16 14:52:45,370   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1285 bytes result sent to driver
2019-06-16 14:52:45,373   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 91 ms on localhost (executor driver) (1/1)
2019-06-16 14:52:45,373   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 14:52:45,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (reduceByKey at SparkScalaWordCount.scala:49) finished in 0.111 s
2019-06-16 14:52:45,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:52:45,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:52:45,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 14:52:45,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:52:45,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ShuffledRDD[5] at sortByKey at SparkScalaWordCount.scala:50), which has no missing parents
2019-06-16 14:52:45,379   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 4.1 GB)
2019-06-16 14:52:45,381   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
2019-06-16 14:52:45,382   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:61319 (size: 2.1 KB, free: 4.1 GB)
2019-06-16 14:52:45,383   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:52:45,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ShuffledRDD[5] at sortByKey at SparkScalaWordCount.scala:50) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:52:45,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 14:52:45,389   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 14:52:45,389   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 14:52:45,395   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:52:45,396   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 14:52:45,427   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1095 bytes result sent to driver
2019-06-16 14:52:45,429   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 42 ms on localhost (executor driver) (1/1)
2019-06-16 14:52:45,429   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 14:52:45,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:50) finished in 0.053 s
2019-06-16 14:52:45,438   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:50, took 0.930199 s
2019-06-16 14:52:45,445   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:52:45,452   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@f08fdce{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:52:45,455   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:52:45,467   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:52:45,482   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:52:45,483   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:52:45,490   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:52:45,492   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:52:45,496   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:52:45,497   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:52:45,498   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-dc85a536-e212-4dc1-b97c-3f176fa5b3e5
2019-06-16 14:58:23,287   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:58:24,059   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:58:24,178   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:58:24,179   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:58:24,180   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:58:24,181   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:58:24,182   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:58:25,019   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61400.
2019-06-16 14:58:25,048   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:58:25,074   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:58:25,079   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:58:25,079   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:58:25,091   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-581d5b68-e972-4418-a40e-041a9f7fc449
2019-06-16 14:58:25,123   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:58:25,141   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:58:25,234   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3351ms
2019-06-16 14:58:25,300   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:58:25,317   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3436ms
2019-06-16 14:58:25,338   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@3d0ace81{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:58:25,339   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:58:25,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,371   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,371   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,373   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,374   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,374   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,375   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,376   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,376   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,377   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,378   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,380   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,380   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,381   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,382   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,383   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,384   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,385   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,386   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,393   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,395   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,397   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:58:25,399   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:58:25,554   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:58:25,583   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61413.
2019-06-16 14:58:25,584   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:61413
2019-06-16 14:58:25,585   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:58:25,587   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 61413, None)
2019-06-16 14:58:25,591   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:61413 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 61413, None)
2019-06-16 14:58:25,597   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 61413, None)
2019-06-16 14:58:25,598   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 61413, None)
2019-06-16 14:58:25,791   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:58:26,422   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:58:26,519   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:58:26,522   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:61413 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:58:26,530   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:47
2019-06-16 14:58:26,672   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 14:58:26,727   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:54
2019-06-16 14:58:27,083   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:49)
2019-06-16 14:58:27,083   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at SparkScalaWordCount.scala:51)
2019-06-16 14:58:27,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:54) with 1 output partitions
2019-06-16 14:58:27,086   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:54)
2019-06-16 14:58:27,086   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 14:58:27,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 14:58:27,099   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49), which has no missing parents
2019-06-16 14:58:27,167   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 14:58:27,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 14:58:27,171   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:61413 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 14:58:27,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:58:27,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:58:27,192   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 14:58:27,243   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 14:58:27,251   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 14:58:27,318   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 14:58:27,416   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 14:58:27,426   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 197 ms on localhost (executor driver) (1/1)
2019-06-16 14:58:27,428   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 14:58:27,436   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:49) finished in 0.313 s
2019-06-16 14:58:27,436   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:58:27,437   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:58:27,438   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 14:58:27,438   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:58:27,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:51), which has no missing parents
2019-06-16 14:58:27,459   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 4.1 GB)
2019-06-16 14:58:27,461   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 14:58:27,462   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:61413 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 14:58:27,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:58:27,465   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:51) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:58:27,465   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 14:58:27,471   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 14:58:27,471   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 14:58:27,494   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:58:27,496   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 14:58:27,549   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1285 bytes result sent to driver
2019-06-16 14:58:27,552   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 82 ms on localhost (executor driver) (1/1)
2019-06-16 14:58:27,552   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 14:58:27,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:51) finished in 0.104 s
2019-06-16 14:58:27,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:58:27,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:58:27,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 14:58:27,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:58:27,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:51), which has no missing parents
2019-06-16 14:58:27,559   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 4.1 GB)
2019-06-16 14:58:27,561   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2040.0 B, free 4.1 GB)
2019-06-16 14:58:27,562   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:61413 (size: 2040.0 B, free: 4.1 GB)
2019-06-16 14:58:27,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:58:27,565   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:51) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:58:27,565   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 14:58:27,568   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 14:58:27,569   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 14:58:27,576   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:58:27,577   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 14:58:27,595   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1095 bytes result sent to driver
2019-06-16 14:58:27,598   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 32 ms on localhost (executor driver) (1/1)
2019-06-16 14:58:27,598   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 14:58:27,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:54) finished in 0.042 s
2019-06-16 14:58:27,607   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:54, took 0.878964 s
2019-06-16 14:58:27,613   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:58:27,620   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@3d0ace81{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:58:27,623   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:58:27,635   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:58:27,649   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:58:27,650   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:58:27,657   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:58:27,660   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:58:27,665   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:58:27,666   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:58:27,666   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-b74eca49-39a6-4a3f-af54-d6e85cea782d
2019-06-16 14:59:31,669   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 14:59:32,416   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 14:59:32,527   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 14:59:32,529   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 14:59:32,530   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 14:59:32,531   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 14:59:32,531   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 14:59:33,424   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61463.
2019-06-16 14:59:33,457   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 14:59:33,484   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 14:59:33,489   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 14:59:33,490   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 14:59:33,503   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-228f7a2d-f9ac-4f5e-b96b-36ca3f7144ec
2019-06-16 14:59:33,538   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 14:59:33,558   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 14:59:33,660   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3422ms
2019-06-16 14:59:33,733   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 14:59:33,752   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3516ms
2019-06-16 14:59:33,776   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@433da62d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:59:33,776   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 14:59:33,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,811   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,811   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,815   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,815   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,817   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,819   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,820   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,822   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,825   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,827   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,828   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,835   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,836   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,837   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,838   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 14:59:33,842   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 14:59:33,995   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 14:59:34,027   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61476.
2019-06-16 14:59:34,028   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:61476
2019-06-16 14:59:34,029   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 14:59:34,031   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 61476, None)
2019-06-16 14:59:34,035   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:61476 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 61476, None)
2019-06-16 14:59:34,040   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 61476, None)
2019-06-16 14:59:34,041   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 61476, None)
2019-06-16 14:59:34,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 14:59:34,891   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 14:59:34,983   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 14:59:34,986   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:61476 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 14:59:34,993   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:47
2019-06-16 14:59:35,132   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 14:59:35,197   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:54
2019-06-16 14:59:35,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:49)
2019-06-16 14:59:35,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at SparkScalaWordCount.scala:51)
2019-06-16 14:59:35,556   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:54) with 1 output partitions
2019-06-16 14:59:35,557   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:54)
2019-06-16 14:59:35,557   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 14:59:35,560   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 14:59:35,569   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49), which has no missing parents
2019-06-16 14:59:35,639   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 14:59:35,641   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 14:59:35,642   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:61476 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 14:59:35,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:59:35,663   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:49) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:59:35,664   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 14:59:35,714   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 14:59:35,727   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 14:59:35,798   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 14:59:35,923   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 14:59:35,933   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 232 ms on localhost (executor driver) (1/1)
2019-06-16 14:59:35,936   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 14:59:35,944   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:49) finished in 0.353 s
2019-06-16 14:59:35,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:59:35,946   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:59:35,946   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 14:59:35,947   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:59:35,951   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:51), which has no missing parents
2019-06-16 14:59:35,969   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 4.1 GB)
2019-06-16 14:59:35,972   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 14:59:35,973   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:61476 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 14:59:35,974   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:59:35,976   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:51) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:59:35,976   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 14:59:35,982   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 14:59:35,982   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 14:59:36,004   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:59:36,007   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-06-16 14:59:36,054   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1328 bytes result sent to driver
2019-06-16 14:59:36,056   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 76 ms on localhost (executor driver) (1/1)
2019-06-16 14:59:36,056   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 14:59:36,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:51) finished in 0.099 s
2019-06-16 14:59:36,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 14:59:36,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 14:59:36,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 14:59:36,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 14:59:36,059   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:51), which has no missing parents
2019-06-16 14:59:36,063   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 4.1 GB)
2019-06-16 14:59:36,066   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2019-06-16 14:59:36,067   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:61476 (size: 2.0 KB, free: 4.1 GB)
2019-06-16 14:59:36,067   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 14:59:36,070   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:51) (first 15 tasks are for partitions Vector(0))
2019-06-16 14:59:36,070   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 14:59:36,073   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 14:59:36,073   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 14:59:36,080   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 14:59:36,081   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 14:59:36,099   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1181 bytes result sent to driver
2019-06-16 14:59:36,101   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 30 ms on localhost (executor driver) (1/1)
2019-06-16 14:59:36,101   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 14:59:36,102   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:54) finished in 0.041 s
2019-06-16 14:59:36,110   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:54, took 0.912022 s
2019-06-16 14:59:36,115   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 14:59:36,122   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@433da62d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 14:59:36,124   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 14:59:36,136   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 14:59:36,150   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 14:59:36,151   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 14:59:36,158   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 14:59:36,161   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 14:59:36,165   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 14:59:36,166   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 14:59:36,166   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e6fbc2d5-0c0c-4516-a5ee-ed218ff82a80
2019-06-16 15:00:39,258   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:00:39,949   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordcount
2019-06-16 15:00:40,060   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:00:40,061   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:00:40,062   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:00:40,063   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:00:40,064   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:00:40,962   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61519.
2019-06-16 15:00:40,993   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:00:41,018   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:00:41,022   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:00:41,023   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:00:41,036   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-2662cfb4-1845-4891-8294-b8f4c8b7ed45
2019-06-16 15:00:41,067   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:00:41,085   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:00:41,177   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3366ms
2019-06-16 15:00:41,244   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:00:41,260   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3450ms
2019-06-16 15:00:41,282   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@4e8d16ae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:00:41,282   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:00:41,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,316   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,317   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,327   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,328   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,329   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,330   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/static,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/api,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:00:41,342   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:00:41,500   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:00:41,531   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61532.
2019-06-16 15:00:41,532   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:61532
2019-06-16 15:00:41,534   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:00:41,536   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 61532, None)
2019-06-16 15:00:41,539   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:61532 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 61532, None)
2019-06-16 15:00:41,543   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 61532, None)
2019-06-16 15:00:41,544   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 61532, None)
2019-06-16 15:00:41,725   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3b48e183{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:00:42,323   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:00:42,412   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:00:42,416   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:61532 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:00:42,421   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:52
2019-06-16 15:00:42,553   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:00:42,596   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:57
2019-06-16 15:00:42,958   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:54)
2019-06-16 15:00:42,959   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 4 (reduceByKey at SparkScalaWordCount.scala:54)
2019-06-16 15:00:42,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:57) with 1 output partitions
2019-06-16 15:00:42,963   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:57)
2019-06-16 15:00:42,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 15:00:42,967   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 15:00:42,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:54), which has no missing parents
2019-06-16 15:00:43,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:00:43,055   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:00:43,056   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:61532 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:00:43,057   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:00:43,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:54) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:00:43,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:00:43,128   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:00:43,137   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:00:43,211   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:00:43,311   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:00:43,321   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 206 ms on localhost (executor driver) (1/1)
2019-06-16 15:00:43,324   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:00:43,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:54) finished in 0.328 s
2019-06-16 15:00:43,332   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:00:43,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:00:43,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 15:00:43,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:00:43,337   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ShuffledRDD[4] at reduceByKey at SparkScalaWordCount.scala:54), which has no missing parents
2019-06-16 15:00:43,353   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 4.1 GB)
2019-06-16 15:00:43,356   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.1 GB)
2019-06-16 15:00:43,357   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:61532 (size: 2.2 KB, free: 4.1 GB)
2019-06-16 15:00:43,358   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:00:43,359   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ShuffledRDD[4] at reduceByKey at SparkScalaWordCount.scala:54) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:00:43,360   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:00:43,365   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 15:00:43,365   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:00:43,384   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:00:43,386   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 15:00:43,449   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1285 bytes result sent to driver
2019-06-16 15:00:43,451   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 88 ms on localhost (executor driver) (1/1)
2019-06-16 15:00:43,451   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:00:43,452   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (reduceByKey at SparkScalaWordCount.scala:54) finished in 0.109 s
2019-06-16 15:00:43,452   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:00:43,453   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:00:43,453   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 15:00:43,453   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:00:43,453   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ShuffledRDD[5] at sortByKey at SparkScalaWordCount.scala:55), which has no missing parents
2019-06-16 15:00:43,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 4.1 GB)
2019-06-16 15:00:43,459   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
2019-06-16 15:00:43,460   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:61532 (size: 2.1 KB, free: 4.1 GB)
2019-06-16 15:00:43,461   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:00:43,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ShuffledRDD[5] at sortByKey at SparkScalaWordCount.scala:55) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:00:43,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 15:00:43,465   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:00:43,466   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 15:00:43,472   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:00:43,472   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-06-16 15:00:43,501   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1095 bytes result sent to driver
2019-06-16 15:00:43,503   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 39 ms on localhost (executor driver) (1/1)
2019-06-16 15:00:43,503   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 15:00:43,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:57) finished in 0.048 s
2019-06-16 15:00:43,511   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:57, took 0.914631 s
2019-06-16 15:00:43,516   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 15:00:43,523   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@4e8d16ae{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:00:43,525   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:00:43,536   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:00:43,549   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:00:43,550   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:00:43,557   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:00:43,560   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:00:43,564   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:00:43,564   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:00:43,565   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4bb6af03-4997-4581-bb3a-5985b9825b20
2019-06-16 15:41:42,785   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:41:43,503   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wc
2019-06-16 15:41:43,607   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:41:43,608   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:41:43,609   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:41:43,610   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:41:43,611   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:41:44,541   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 62147.
2019-06-16 15:41:44,571   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:41:44,596   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:41:44,601   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:41:44,601   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:41:44,614   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-39b37e78-3f44-4181-a067-9f3ecd3a2771
2019-06-16 15:41:44,645   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:41:44,664   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:41:44,760   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3373ms
2019-06-16 15:41:44,829   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:41:44,846   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3460ms
2019-06-16 15:41:44,868   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@7ed302d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:41:44,868   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:41:44,898   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5e671e20{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,899   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@61af1510{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,900   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@37af1f93{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,903   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3901f6af{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,904   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@602ae7b6{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,905   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,907   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,908   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,910   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,911   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,913   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,914   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,915   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,916   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7487b142{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7139bd31{/static,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,923   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bfc14b9{/,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fb6097b{/api,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,925   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@55d9b8f0{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,926   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a518813{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:41:44,928   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:41:45,087   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:41:45,118   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62160.
2019-06-16 15:41:45,119   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:62160
2019-06-16 15:41:45,121   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:41:45,123   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 62160, None)
2019-06-16 15:41:45,125   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:62160 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 62160, None)
2019-06-16 15:41:45,130   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 62160, None)
2019-06-16 15:41:45,131   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 62160, None)
2019-06-16 15:41:45,327   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30c1da48{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:41:45,956   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:41:46,046   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:41:46,050   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:62160 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:41:46,057   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:89
2019-06-16 15:41:46,197   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:41:46,256   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:90
2019-06-16 15:41:46,628   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:90)
2019-06-16 15:41:46,629   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at SparkScalaWordCount.scala:90)
2019-06-16 15:41:46,632   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:90) with 1 output partitions
2019-06-16 15:41:46,634   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:90)
2019-06-16 15:41:46,635   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 15:41:46,638   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 15:41:46,649   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:41:46,722   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:41:46,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:41:46,725   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:62160 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:41:46,727   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:41:46,746   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:41:46,747   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:41:46,797   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:41:46,805   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:41:46,876   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:41:46,975   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:41:46,990   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 203 ms on localhost (executor driver) (1/1)
2019-06-16 15:41:46,998   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:41:47,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:90) finished in 0.330 s
2019-06-16 15:41:47,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:41:47,010   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:41:47,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 15:41:47,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:41:47,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:41:47,033   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 4.1 GB)
2019-06-16 15:41:47,036   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 15:41:47,037   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:62160 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 15:41:47,038   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:41:47,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:41:47,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:41:47,044   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 15:41:47,045   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:41:47,067   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:41:47,069   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 15:41:47,112   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1285 bytes result sent to driver
2019-06-16 15:41:47,114   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 72 ms on localhost (executor driver) (1/1)
2019-06-16 15:41:47,114   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:41:47,115   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:90) finished in 0.093 s
2019-06-16 15:41:47,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:41:47,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:41:47,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 15:41:47,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:41:47,117   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:41:47,121   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 4.1 GB)
2019-06-16 15:41:47,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2036.0 B, free 4.1 GB)
2019-06-16 15:41:47,124   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:62160 (size: 2036.0 B, free: 4.1 GB)
2019-06-16 15:41:47,125   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:41:47,126   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:41:47,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 15:41:47,130   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:41:47,130   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 15:41:47,137   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:41:47,138   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 15:41:47,156   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1138 bytes result sent to driver
2019-06-16 15:41:47,158   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 30 ms on localhost (executor driver) (1/1)
2019-06-16 15:41:47,159   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 15:41:47,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:90) finished in 0.041 s
2019-06-16 15:41:47,167   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:90, took 0.910120 s
2019-06-16 15:41:47,173   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-06-16 15:41:47,180   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@7ed302d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:41:47,183   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:41:47,196   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:41:47,208   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:41:47,209   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:41:47,216   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:41:47,219   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:41:47,223   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:41:47,224   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:41:47,225   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-6f8e59da-1574-45ee-92d2-76e090267089
2019-06-16 15:45:04,907   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:45:05,629   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wc
2019-06-16 15:45:05,733   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:45:05,734   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:45:05,735   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:45:05,736   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:45:05,737   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:45:06,629   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 62220.
2019-06-16 15:45:06,662   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:45:06,689   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:45:06,694   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:45:06,695   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:45:06,708   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-7ebb0a15-915e-42f1-81a1-b5ea9fc5f25b
2019-06-16 15:45:06,743   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:45:06,763   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:45:06,864   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3425ms
2019-06-16 15:45:06,936   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:45:06,955   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3517ms
2019-06-16 15:45:06,978   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6ec47ae2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:45:06,978   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:45:07,012   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fe7f967{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,013   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5160e6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,014   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@778d82e9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,016   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,017   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,019   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71ad3d8a{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,020   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,021   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,022   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,024   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,024   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,026   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,029   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/static,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,039   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d192aef{/,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/api,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dfe5525{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,042   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:45:07,044   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:45:07,199   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:45:07,229   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62233.
2019-06-16 15:45:07,230   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:62233
2019-06-16 15:45:07,231   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:45:07,234   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 62233, None)
2019-06-16 15:45:07,238   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:62233 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 62233, None)
2019-06-16 15:45:07,243   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 62233, None)
2019-06-16 15:45:07,243   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 62233, None)
2019-06-16 15:45:07,445   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5434e40c{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:45:08,044   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:45:08,135   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:45:08,138   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:62233 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:45:08,145   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:94
2019-06-16 15:45:08,273   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:45:08,333   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:94
2019-06-16 15:45:08,736   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:94)
2019-06-16 15:45:08,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at SparkScalaWordCount.scala:94)
2019-06-16 15:45:08,740   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:94) with 1 output partitions
2019-06-16 15:45:08,741   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:94)
2019-06-16 15:45:08,741   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 15:45:08,744   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 15:45:08,753   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:94), which has no missing parents
2019-06-16 15:45:08,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:45:08,836   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:45:08,837   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:62233 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:45:08,838   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:45:08,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:94) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:45:08,860   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:45:08,916   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:45:08,924   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:45:08,997   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:45:09,107   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:45:09,117   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 218 ms on localhost (executor driver) (1/1)
2019-06-16 15:45:09,120   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:45:09,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:94) finished in 0.350 s
2019-06-16 15:45:09,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:45:09,129   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:45:09,129   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 15:45:09,130   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:45:09,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:94), which has no missing parents
2019-06-16 15:45:09,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 4.1 GB)
2019-06-16 15:45:09,156   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 15:45:09,157   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:62233 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 15:45:09,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:45:09,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at SparkScalaWordCount.scala:94) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:45:09,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:45:09,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 15:45:09,165   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:45:09,187   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:45:09,189   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 15:45:09,233   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1328 bytes result sent to driver
2019-06-16 15:45:09,235   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 72 ms on localhost (executor driver) (1/1)
2019-06-16 15:45:09,235   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:45:09,237   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:94) finished in 0.094 s
2019-06-16 15:45:09,237   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:45:09,237   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:45:09,237   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 15:45:09,237   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:45:09,238   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:94), which has no missing parents
2019-06-16 15:45:09,242   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 4.1 GB)
2019-06-16 15:45:09,245   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2019-06-16 15:45:09,245   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:62233 (size: 2.0 KB, free: 4.1 GB)
2019-06-16 15:45:09,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:45:09,249   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at SparkScalaWordCount.scala:94) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:45:09,249   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 15:45:09,250   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:45:09,251   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 15:45:09,259   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:45:09,259   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 15:45:09,278   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1138 bytes result sent to driver
2019-06-16 15:45:09,280   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 30 ms on localhost (executor driver) (1/1)
2019-06-16 15:45:09,280   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 15:45:09,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:94) finished in 0.041 s
2019-06-16 15:45:09,289   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:94, took 0.956080 s
2019-06-16 15:45:09,298   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6ec47ae2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:45:09,301   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:45:09,313   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:45:09,330   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:45:09,330   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:45:09,336   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:45:09,339   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:45:09,347   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:45:09,350   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:45:09,351   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-2737c767-bc8c-4dae-8c54-4834cde5dd34
2019-06-16 15:47:46,828   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:47:47,613   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wc
2019-06-16 15:47:47,718   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:47:47,719   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:47:47,720   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:47:47,721   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:47:47,722   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:47:48,621   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 62287.
2019-06-16 15:47:48,651   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:47:48,676   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:47:48,680   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:47:48,681   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:47:48,694   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d32b5312-811e-486f-9b39-796ff2baf6d0
2019-06-16 15:47:48,725   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:47:48,744   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:47:48,839   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3384ms
2019-06-16 15:47:48,908   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:47:48,924   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3471ms
2019-06-16 15:47:48,946   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@21fcf254{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:47:48,947   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:47:48,977   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,979   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,980   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,981   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,982   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,982   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,985   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,988   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,989   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,991   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,992   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,993   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:47:48,994   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/static,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/api,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,007   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:47:49,166   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:47:49,199   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62300.
2019-06-16 15:47:49,200   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:62300
2019-06-16 15:47:49,202   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:47:49,204   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 62300, None)
2019-06-16 15:47:49,207   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:62300 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 62300, None)
2019-06-16 15:47:49,211   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 62300, None)
2019-06-16 15:47:49,212   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 62300, None)
2019-06-16 15:47:49,397   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3b48e183{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:47:49,973   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:47:50,062   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:47:50,066   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:62300 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:47:50,073   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:90
2019-06-16 15:47:50,206   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:47:50,278   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:91
2019-06-16 15:47:50,688   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:90)
2019-06-16 15:47:50,690   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 6 (sortBy at SparkScalaWordCount.scala:91)
2019-06-16 15:47:50,693   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:91) with 1 output partitions
2019-06-16 15:47:50,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:91)
2019-06-16 15:47:50,695   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 15:47:50,697   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 15:47:50,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:47:50,791   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:47:50,793   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:47:50,794   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:62300 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:47:50,796   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:47:50,822   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:47:50,823   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:47:50,875   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:47:50,885   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:47:50,964   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:47:51,105   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:47:51,119   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 258 ms on localhost (executor driver) (1/1)
2019-06-16 15:47:51,124   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:47:51,132   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:90) finished in 0.396 s
2019-06-16 15:47:51,133   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:47:51,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:47:51,135   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 15:47:51,135   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:47:51,141   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at sortBy at SparkScalaWordCount.scala:91), which has no missing parents
2019-06-16 15:47:51,162   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.5 KB, free 4.1 GB)
2019-06-16 15:47:51,171   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 15:47:51,172   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:62300 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 15:47:51,173   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:47:51,175   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at sortBy at SparkScalaWordCount.scala:91) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:47:51,175   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:47:51,181   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 15:47:51,182   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:47:51,208   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:47:51,209   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 15:47:51,254   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1328 bytes result sent to driver
2019-06-16 15:47:51,256   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 77 ms on localhost (executor driver) (1/1)
2019-06-16 15:47:51,256   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:47:51,257   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:91) finished in 0.108 s
2019-06-16 15:47:51,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:47:51,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:47:51,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 15:47:51,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:47:51,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at map at SparkScalaWordCount.scala:91), which has no missing parents
2019-06-16 15:47:51,262   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 4.1 GB)
2019-06-16 15:47:51,265   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 4.1 GB)
2019-06-16 15:47:51,266   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:62300 (size: 2.0 KB, free: 4.1 GB)
2019-06-16 15:47:51,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:47:51,268   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at SparkScalaWordCount.scala:91) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:47:51,269   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 15:47:51,271   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:47:51,272   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 15:47:51,279   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:47:51,280   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-06-16 15:47:51,299   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1095 bytes result sent to driver
2019-06-16 15:47:51,301   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 31 ms on localhost (executor driver) (1/1)
2019-06-16 15:47:51,301   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 15:47:51,303   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:91) finished in 0.041 s
2019-06-16 15:47:51,310   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:91, took 1.031723 s
2019-06-16 15:47:51,321   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@21fcf254{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:47:51,324   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:47:51,335   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:47:51,363   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:47:51,363   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:47:51,371   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:47:51,373   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:47:51,380   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:47:51,383   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:47:51,384   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-18e54d36-3174-4b6d-909c-276f9f8cfc85
2019-06-16 15:49:13,433   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:49:14,165   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wc
2019-06-16 15:49:14,270   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:49:14,272   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:49:14,273   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:49:14,273   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:49:14,274   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:49:15,261   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 62348.
2019-06-16 15:49:15,294   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:49:15,320   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:49:15,324   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:49:15,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:49:15,337   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-9da395eb-d35d-465e-b18f-9bd587d08d6d
2019-06-16 15:49:15,369   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:49:15,387   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:49:15,480   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3515ms
2019-06-16 15:49:15,546   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:49:15,564   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3600ms
2019-06-16 15:49:15,585   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18eff9b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:49:15,585   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:49:15,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,617   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,618   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,619   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,620   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,621   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,622   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,624   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,626   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,627   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,628   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,629   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,631   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,632   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,633   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,640   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/static,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,641   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,642   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/api,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,643   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:49:15,646   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:49:15,797   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:49:15,828   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62361.
2019-06-16 15:49:15,829   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:62361
2019-06-16 15:49:15,831   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:49:15,833   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 62361, None)
2019-06-16 15:49:15,836   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:62361 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 62361, None)
2019-06-16 15:49:15,841   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 62361, None)
2019-06-16 15:49:15,842   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 62361, None)
2019-06-16 15:49:16,039   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3b48e183{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:16,649   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:49:16,737   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:49:16,741   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:62361 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:49:16,748   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:90
2019-06-16 15:49:16,878   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:49:16,926   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:91
2019-06-16 15:49:17,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:90)
2019-06-16 15:49:17,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:91) with 1 output partitions
2019-06-16 15:49:17,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (foreach at SparkScalaWordCount.scala:91)
2019-06-16 15:49:17,280   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-06-16 15:49:17,282   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-06-16 15:49:17,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:49:17,357   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:49:17,360   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:49:17,361   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:62361 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:49:17,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:49:17,380   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:49:17,381   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:49:17,432   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:49:17,440   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:49:17,509   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:49:17,627   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:49:17,637   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 220 ms on localhost (executor driver) (1/1)
2019-06-16 15:49:17,640   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:49:17,648   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:90) finished in 0.337 s
2019-06-16 15:49:17,649   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:49:17,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:49:17,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-06-16 15:49:17,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:49:17,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:91), which has no missing parents
2019-06-16 15:49:17,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 4.1 GB)
2019-06-16 15:49:17,669   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
2019-06-16 15:49:17,670   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:62361 (size: 2.1 KB, free: 4.1 GB)
2019-06-16 15:49:17,671   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:49:17,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at map at SparkScalaWordCount.scala:91) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:49:17,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:49:17,680   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:49:17,680   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:49:17,700   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:49:17,702   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-06-16 15:49:17,742   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1138 bytes result sent to driver
2019-06-16 15:49:17,744   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 67 ms on localhost (executor driver) (1/1)
2019-06-16 15:49:17,744   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:49:17,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (foreach at SparkScalaWordCount.scala:91) finished in 0.082 s
2019-06-16 15:49:17,751   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:91, took 0.824701 s
2019-06-16 15:49:17,761   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18eff9b7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:49:17,763   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:49:17,775   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:49:17,787   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:49:17,788   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:49:17,794   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:49:17,797   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:49:17,805   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:49:17,808   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:49:17,809   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-9fb54d58-8f0e-4c4d-8d4c-3bc55a03537c
2019-06-16 15:49:40,211   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.3.0
2019-06-16 15:49:40,933   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wc
2019-06-16 15:49:41,041   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: Administrator
2019-06-16 15:49:41,042   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: Administrator
2019-06-16 15:49:41,043   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-06-16 15:49:41,043   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-06-16 15:49:41,044   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2019-06-16 15:49:41,937   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 62403.
2019-06-16 15:49:41,970   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-06-16 15:49:41,998   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-06-16 15:49:42,002   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-06-16 15:49:42,003   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-06-16 15:49:42,017   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-495fece7-e10e-4d52-948e-7a9401b30985
2019-06-16 15:49:42,052   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 4.1 GB
2019-06-16 15:49:42,073   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-06-16 15:49:42,175   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3387ms
2019-06-16 15:49:42,250   INFO --- [main]  org.spark_project.jetty.server.Server(line:346) : jetty-9.3.z-SNAPSHOT
2019-06-16 15:49:42,268   INFO --- [main]  org.spark_project.jetty.server.Server(line:414) : Started @3481ms
2019-06-16 15:49:42,291   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6d195ddd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:49:42,291   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-06-16 15:49:42,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59e43e8c{/jobs,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,327   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2eadc9f6{/jobs/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,328   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2903c6ff{/jobs/job,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,329   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@408e96d9{/jobs/job/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,330   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@59901c4d{/stages,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,331   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@168cd36b{/stages/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,332   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d8d9199{/stages/stage,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,333   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47af099e{/stages/stage/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,333   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@700f518a{/stages/pool,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b835727{/stages/pool/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13da7ab0{/storage,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c8662ac{/storage/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260ff5b7{/storage/rdd,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,338   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3724b43e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77eb5790{/environment,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68e7c8c3{/environment/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@319c3a25{/executors,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238bfd6c{/executors/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,342   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ef1695a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,343   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58860997{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@81b5db0{/static,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,353   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1416cf9f{/,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,354   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@84487f4{/api,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1290c49{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a9b9909{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-06-16 15:49:42,358   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://USER-20180114AD:4040
2019-06-16 15:49:42,510   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-06-16 15:49:42,543   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62416.
2019-06-16 15:49:42,544   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on USER-20180114AD:62416
2019-06-16 15:49:42,546   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-06-16 15:49:42,548   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, USER-20180114AD, 62416, None)
2019-06-16 15:49:42,552   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager USER-20180114AD:62416 with 4.1 GB RAM, BlockManagerId(driver, USER-20180114AD, 62416, None)
2019-06-16 15:49:42,557   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, USER-20180114AD, 62416, None)
2019-06-16 15:49:42,558   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, USER-20180114AD, 62416, None)
2019-06-16 15:49:42,758   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3b48e183{/metrics/json,null,AVAILABLE,@Spark}
2019-06-16 15:49:43,369   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 4.1 GB)
2019-06-16 15:49:43,462   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 4.1 GB)
2019-06-16 15:49:43,465   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on USER-20180114AD:62416 (size: 20.4 KB, free: 4.1 GB)
2019-06-16 15:49:43,473   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at SparkScalaWordCount.scala:90
2019-06-16 15:49:43,605   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-06-16 15:49:43,669   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at SparkScalaWordCount.scala:91
2019-06-16 15:49:44,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at SparkScalaWordCount.scala:90)
2019-06-16 15:49:44,049   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 6 (sortBy at SparkScalaWordCount.scala:91)
2019-06-16 15:49:44,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at SparkScalaWordCount.scala:91) with 1 output partitions
2019-06-16 15:49:44,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (foreach at SparkScalaWordCount.scala:91)
2019-06-16 15:49:44,053   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-06-16 15:49:44,056   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-06-16 15:49:44,066   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90), which has no missing parents
2019-06-16 15:49:44,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 4.1 GB)
2019-06-16 15:49:44,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 4.1 GB)
2019-06-16 15:49:44,140   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on USER-20180114AD:62416 (size: 2.8 KB, free: 4.1 GB)
2019-06-16 15:49:44,141   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:49:44,162   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at SparkScalaWordCount.scala:90) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:49:44,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-06-16 15:49:44,218   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes)
2019-06-16 15:49:44,228   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-06-16 15:49:44,299   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/workspace2/BigData-Learning/Spark-Learning/src/resources/data/words:0+127
2019-06-16 15:49:44,411   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1156 bytes result sent to driver
2019-06-16 15:49:44,430   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 226 ms on localhost (executor driver) (1/1)
2019-06-16 15:49:44,433   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-06-16 15:49:44,442   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at SparkScalaWordCount.scala:90) finished in 0.352 s
2019-06-16 15:49:44,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:49:44,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:49:44,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-06-16 15:49:44,445   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:49:44,449   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at sortBy at SparkScalaWordCount.scala:91), which has no missing parents
2019-06-16 15:49:44,468   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.5 KB, free 4.1 GB)
2019-06-16 15:49:44,470   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 4.1 GB)
2019-06-16 15:49:44,472   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on USER-20180114AD:62416 (size: 2.5 KB, free: 4.1 GB)
2019-06-16 15:49:44,473   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:49:44,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at sortBy at SparkScalaWordCount.scala:91) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:49:44,475   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-06-16 15:49:44,481   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)
2019-06-16 15:49:44,481   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-06-16 15:49:44,504   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:49:44,506   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 6 ms
2019-06-16 15:49:44,553   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1285 bytes result sent to driver
2019-06-16 15:49:44,556   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 76 ms on localhost (executor driver) (1/1)
2019-06-16 15:49:44,556   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-06-16 15:49:44,557   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at SparkScalaWordCount.scala:91) finished in 0.101 s
2019-06-16 15:49:44,557   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-06-16 15:49:44,558   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-06-16 15:49:44,558   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-06-16 15:49:44,558   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-06-16 15:49:44,558   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[8] at sortBy at SparkScalaWordCount.scala:91), which has no missing parents
2019-06-16 15:49:44,563   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 4.1 GB)
2019-06-16 15:49:44,565   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2043.0 B, free 4.1 GB)
2019-06-16 15:49:44,566   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on USER-20180114AD:62416 (size: 2043.0 B, free: 4.1 GB)
2019-06-16 15:49:44,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2019-06-16 15:49:44,570   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at sortBy at SparkScalaWordCount.scala:91) (first 15 tasks are for partitions Vector(0))
2019-06-16 15:49:44,570   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-06-16 15:49:44,573   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-06-16 15:49:44,573   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-06-16 15:49:44,581   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks out of 1 blocks
2019-06-16 15:49:44,581   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-06-16 15:49:44,600   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1181 bytes result sent to driver
2019-06-16 15:49:44,602   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 31 ms on localhost (executor driver) (1/1)
2019-06-16 15:49:44,602   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-06-16 15:49:44,603   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (foreach at SparkScalaWordCount.scala:91) finished in 0.042 s
2019-06-16 15:49:44,611   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at SparkScalaWordCount.scala:91, took 0.940839 s
2019-06-16 15:49:44,623   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6d195ddd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-06-16 15:49:44,625   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://USER-20180114AD:4040
2019-06-16 15:49:44,638   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-06-16 15:49:44,654   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-06-16 15:49:44,654   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-06-16 15:49:44,662   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-06-16 15:49:44,665   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-06-16 15:49:44,673   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-06-16 15:49:44,677   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-06-16 15:49:44,678   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c6443891-2a0d-4833-bf04-39aa4e144738
